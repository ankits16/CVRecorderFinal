{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d259620",
   "metadata": {},
   "source": [
    "reference:\n",
    "https://github.com/MPieter/YOLOv4-CoreML-Converter/blob/master/convert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import io\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (Conv2D, Input, ZeroPadding2D, Add,\n",
    "                                     UpSampling2D, MaxPooling2D, Concatenate)\n",
    "from tensorflow.keras.layers import LeakyReLU, Layer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "\n",
    "import coremltools as ct\n",
    "\n",
    "print(\"TensorFlow version is: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version is: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(Layer):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X_input = Input(input_shape)\n",
    "        >>> X = Mish()(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Mish, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * K.tanh(K.softplus(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mish, self).get_config()\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"yolov4-tiny.cfg\"\n",
    "weights_path = \"yolov4-tiny.weights\"\n",
    "keras_output_path = 'yolov4_keras_mpieter_23rdMay.h5'\n",
    "# yolov2-coco-9.mlmodel\n",
    "output_path  = \"yolov4_keras_mpieter_coreml_23rdMay.mlmodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights and config.\n",
    "print('Loading weights.')\n",
    "weights_file = open(weights_path, 'rb')\n",
    "major, minor, revision = np.ndarray(\n",
    "    shape=(3, ), dtype='int32', buffer=weights_file.read(12))\n",
    "if (major*10+minor) >= 2 and major < 1000 and minor < 1000:\n",
    "    seen = np.ndarray(shape=(1,), dtype='int64',\n",
    "                      buffer=weights_file.read(8))\n",
    "else:\n",
    "    seen = np.ndarray(shape=(1,), dtype='int32',\n",
    "                      buffer=weights_file.read(4))\n",
    "print('Weights Header: ', major, minor, revision, seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb58c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_config_sections(config_file):\n",
    "    \"\"\"Convert all config sections to have unique names.\n",
    "\n",
    "    Adds unique suffixes to config sections for compability with configparser.\n",
    "    \"\"\"\n",
    "    section_counters = defaultdict(int)\n",
    "    output_stream = io.StringIO()\n",
    "    with open(config_file) as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('['):\n",
    "                section = line.strip().strip('[]')\n",
    "                _section = section + '_' + str(section_counters[section])\n",
    "                section_counters[section] += 1\n",
    "                line = line.replace(section, _section)\n",
    "            output_stream.write(line)\n",
    "    output_stream.seek(0)\n",
    "    return output_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parsing Darknet config.')\n",
    "unique_config_file = unique_config_sections(config_path)\n",
    "cfg_parser = configparser.ConfigParser()\n",
    "cfg_parser.read_file(unique_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aa05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(608, 608, 3))\n",
    "prev_layer = input_layer\n",
    "all_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ab967",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = float(cfg_parser['net_0']['decay']) if 'net_0' in cfg_parser.sections() else 5e-4\n",
    "weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "out_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0741e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in cfg_parser.sections():\n",
    "    print('Parsing section {}'.format(section))\n",
    "    if section.startswith('convolutional'):\n",
    "        filters = int(cfg_parser[section]['filters'])\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        pad = int(cfg_parser[section]['pad'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        batch_normalize = 'batch_normalize' in cfg_parser[section]\n",
    "\n",
    "        padding = 'same' if pad == 1 and stride == 1 else 'valid'\n",
    "\n",
    "        # Setting weights.\n",
    "        # Darknet serializes convolutional weights as:\n",
    "        # [bias/beta, [gamma, mean, variance], conv_weights]\n",
    "        prev_layer_shape = K.int_shape(prev_layer)\n",
    "\n",
    "        weights_shape = (size, size, prev_layer_shape[-1], filters)\n",
    "        darknet_w_shape = (filters, weights_shape[2], size, size)\n",
    "        weights_size = np.product(weights_shape)\n",
    "\n",
    "        print('conv2d', 'bn'\n",
    "              if batch_normalize else '  ', activation, weights_shape)\n",
    "\n",
    "        conv_bias = np.ndarray(\n",
    "            shape=(filters, ),\n",
    "            dtype='float32',\n",
    "            buffer=weights_file.read(filters * 4))\n",
    "        count += filters\n",
    "\n",
    "        if batch_normalize:\n",
    "            bn_weights = np.ndarray(\n",
    "                shape=(3, filters),\n",
    "                dtype='float32',\n",
    "                buffer=weights_file.read(filters * 12))\n",
    "            count += 3 * filters\n",
    "\n",
    "            bn_weight_list = [\n",
    "                bn_weights[0],  # scale gamma\n",
    "                conv_bias,  # shift beta\n",
    "                bn_weights[1],  # running mean\n",
    "                bn_weights[2]  # running var\n",
    "            ]\n",
    "\n",
    "        conv_weights = np.ndarray(\n",
    "            shape=darknet_w_shape,\n",
    "            dtype='float32',\n",
    "            buffer=weights_file.read(weights_size * 4))\n",
    "        count += weights_size\n",
    "\n",
    "        # DarkNet conv_weights are serialized Caffe-style:\n",
    "        # (out_dim, in_dim, height, width)\n",
    "        # We would like to set these to Tensorflow order:\n",
    "        # (height, width, in_dim, out_dim)\n",
    "        conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "        conv_weights = [conv_weights] if batch_normalize else [\n",
    "            conv_weights, conv_bias\n",
    "        ]\n",
    "\n",
    "        # Handle activation.\n",
    "        act_fn = None\n",
    "        if activation == 'leaky' or activation == 'mish':\n",
    "            pass  # Add advanced activation later.\n",
    "        elif activation != 'linear':\n",
    "            raise ValueError(\n",
    "                'Unknown activation function `{}` in section {}'.format(\n",
    "                    activation, section))\n",
    "\n",
    "        # Create Conv2D layer\n",
    "        if stride > 1:\n",
    "            # Darknet uses left and top padding instead of 'same' mode\n",
    "            prev_layer = ZeroPadding2D(((1, 0), (1, 0)))(prev_layer)\n",
    "        conv_layer = (Conv2D(\n",
    "            filters, (size, size),\n",
    "            strides=(stride, stride),\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            use_bias=not batch_normalize,\n",
    "            weights=conv_weights,\n",
    "            activation=act_fn,\n",
    "            padding=padding))(prev_layer)\n",
    "\n",
    "        if batch_normalize:\n",
    "            conv_layer = (BatchNormalization(\n",
    "                weights=bn_weight_list))(conv_layer)\n",
    "        prev_layer = conv_layer\n",
    "\n",
    "        if activation == 'linear':\n",
    "            all_layers.append(prev_layer)\n",
    "        elif activation == 'leaky':\n",
    "            act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n",
    "            prev_layer = act_layer\n",
    "            all_layers.append(act_layer)\n",
    "        elif activation == 'mish':\n",
    "            act_layer = Mish()(prev_layer)\n",
    "            prev_layer = act_layer\n",
    "            all_layers.append(act_layer)\n",
    "\n",
    "    elif section.startswith('route'):\n",
    "        ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]\n",
    "        layers = [all_layers[i] for i in ids]\n",
    "        if len(layers) > 1:\n",
    "            print('Concatenating route layers:', layers)\n",
    "            concatenate_layer = Concatenate()(layers)\n",
    "            all_layers.append(concatenate_layer)\n",
    "            prev_layer = concatenate_layer\n",
    "        else:\n",
    "            skip_layer = layers[0]  # only one layer to route\n",
    "            all_layers.append(skip_layer)\n",
    "            prev_layer = skip_layer\n",
    "\n",
    "    elif section.startswith('maxpool'):\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        all_layers.append(\n",
    "            MaxPooling2D(\n",
    "                pool_size=(size, size),\n",
    "                strides=(stride, stride),\n",
    "                padding='same')(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('shortcut'):\n",
    "        index = int(cfg_parser[section]['from'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        assert activation == 'linear', 'Only linear activation supported.'\n",
    "        all_layers.append(Add()([all_layers[index], prev_layer]))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('upsample'):\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        assert stride == 2, 'Only stride=2 supported.'\n",
    "        all_layers.append(UpSampling2D(stride)(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('yolo'):\n",
    "        out_index.append(len(all_layers)-1)\n",
    "        all_layers.append(None)\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('net'):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Unsupported section header type: {}'.format(section))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72038e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save model.\n",
    "if len(out_index) == 0:\n",
    "    out_index.append(len(all_layers)-1)\n",
    "model = Model(inputs=input_layer, outputs=[\n",
    "              all_layers[i] for i in out_index])\n",
    "input_name = model.input_names[0]\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea287b",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.save('{}'.format(keras_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa75ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db6489",
   "metadata": {},
   "source": [
    "# test keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5947b772",
   "metadata": {},
   "source": [
    "# convert to core ml and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(model, inputs=[ct.ImageType(scale=1 / 255.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da20139",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.save(output_path)\n",
    "print('Saved CoreML model to {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coreml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLayers in the converted model:\")\n",
    "for i, layer in enumerate(coreml_model._spec.neuralNetwork.layers):\n",
    "    if layer.HasField(\"custom\"):\n",
    "        print(\"****** Layer %d = %s --> custom layer = %s\" %\n",
    "              (i, layer.name, layer.custom.className))\n",
    "    else:\n",
    "        print(\"Layer %d = %s\" % (i, layer.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274506d",
   "metadata": {},
   "source": [
    "# test keras model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "yolo_model = load_model(keras_output_path, custom_objects={\"Mish\" : Mish()})\n",
    "\n",
    "# from keras.models import load_model, Model\n",
    "# yolo_model = load_model(\"yolov4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(labels_path):\n",
    "    with open(labels_path) as f:\n",
    "        labels = f.readlines()\n",
    "    labels = [c.strip() for c in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://github.com/AlexeyAB/darknet/blob/master/cfg/coco.names\n",
    "labels = read_labels('coco.txt')\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f402389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, interpolation = 'bilinear', target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    \n",
    "    return image, width, height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d68ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the image\n",
    "input_w, input_h = 608, 608\n",
    "photo_filename = 'persons_1.jpeg'\n",
    "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n",
    "print(\"image initial size: \", image_w, image_h)\n",
    "print(\"input image\",image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "yhat = yolo_model.predict(image)\n",
    "print(\"output:\",[a.shape for a in yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818842de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes\n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    " \n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    " \n",
    "        return self.label\n",
    " \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    " \n",
    "        return self.score\n",
    " \n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    " \n",
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w, anchors_nb, scales_x_y):\n",
    "    grid_h, grid_w = netout.shape[:2]  \n",
    "    nb_box = anchors_nb\n",
    "    print(f'netout before reshape {netout.shape}')\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    print(f'netout after reshape {netout.shape}')\n",
    "    nb_class = netout.shape[-1] - 5 # 5 = bx,by,bh,bw,pc\n",
    "    \n",
    "    print(f'nb_class shape {nb_class}')\n",
    "    print(\"grid_h,grid_w: \",grid_h,grid_w)   \n",
    "       \n",
    "    \n",
    "    boxes = []\n",
    "    netout[..., :2] = _sigmoid(netout[..., :2]) # x, y\n",
    "    netout[..., :2] = netout[..., :2]*scales_x_y #- 0.5*(scales_x_y - 1.0) # scale x, y\n",
    "\n",
    "    netout[..., 4:] = _sigmoid(netout[..., 4:]) # objectness + classes probabilities\n",
    "\n",
    "    for i in range(grid_h*grid_w):\n",
    "\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        \n",
    "        \n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "\n",
    "            if(objectness > obj_thresh):\n",
    "                print(\"objectness: \",objectness)                \n",
    "            \n",
    "                # first 4 elements are x, y, w, and hhttp://localhost:8889/notebooks/Documents/code/iOS/learn/CVRecorder/converters/YoloToCoreML%20Converter.ipynb#\n",
    "                x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "                x = (col + x) / grid_w # center position, unit: image width\n",
    "                y = (row + y) / grid_h # center position, unit: image height\n",
    "                w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "                h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height            \n",
    "            \n",
    "                # last elements are class probabilities\n",
    "                classes = objectness*netout[int(row)][col][b][5:]\n",
    "                classes *= classes > obj_thresh\n",
    "                box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)           \n",
    "                boxes.append(box)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eef78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Yolo layers\n",
    "obj_thresh = 0.65\n",
    "anchors = [ [12, 16, 19, 36, 40, 28],[36, 75, 76, 55, 72, 146],[142, 110, 192, 243, 459, 401]]\n",
    "scales_x_y = [1.2, 1.1, 1.05]\n",
    "boxes = list()\n",
    "\n",
    "for i in range(len(anchors)):\n",
    "    # decode the output of the network\n",
    "    print(f'<<<<<<<<<<<<<<<< start decoding for achor set {i} >>>>>>>>>>>>>>>>>>>>>')\n",
    "    boxes += decode_netout(yhat[i][0], anchors[i], obj_thresh, input_h, input_w, len(anchors), scales_x_y[i])\n",
    "    print(f'<<<<<<<<<<<<<<<< end decoding for achor set {i} >>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "print(\"nb boxes detected; \",len(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the boxes according the inital size of the image\n",
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    " new_w, new_h = net_w, net_h\n",
    " for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress the non Maximal boxes\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3\n",
    " \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    return float(intersect) / union\n",
    "\n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_nms(boxes, 0.5)\n",
    "print(\"nb boxes remaining; \",len(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate colors in bounding boxes\n",
    "import colorsys\n",
    "import random\n",
    "\n",
    "def generate_colors(class_names):\n",
    "    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] ), int(x[1] ), int(x[2] )), colors))\n",
    "    random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "    random.seed(None)  # Reset seed to default.\n",
    "    return colors\n",
    "\n",
    "# get all of the results above a threshold\n",
    "def get_boxes(boxes, labels, thresh, colors):\n",
    "    v_boxes, v_labels, v_scores, v_colors = list(), list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                v_colors.append(colors[i])\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores, v_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the details of the detected objects for a threshold > 0.25\n",
    "class_threshold = 0.25\n",
    "colors = generate_colors(labels)\n",
    "v_boxes, v_labels, v_scores, v_colors = get_boxes(boxes, labels, class_threshold, colors)\n",
    "print(\"nb boxes remaining; \",len(v_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the result\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# draw all results\n",
    "def draw_boxes(filename, v_boxes, v_labels, v_scores, v_colors):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # plot each box\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        # get coordinates\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        # calculate width and height of the box\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        # create the shape\n",
    "        rect = Rectangle((x1, y1), width, height, fill=False, color=v_colors[i])\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "        # draw text and score in top left corner\n",
    "        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
    "        pyplot.text(x1, y1, label, color='white')\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i],  v_boxes[i].xmin, v_boxes[i].xmax, v_boxes[i].ymin, v_boxes[i].ymax)\n",
    "# draw what we found\n",
    "draw_boxes(photo_filename, v_boxes, v_labels, v_scores, v_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e6ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "903aef08",
   "metadata": {},
   "source": [
    "# load and test core ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_coreml_model = ct.models.MLModel(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_coreml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798d9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
