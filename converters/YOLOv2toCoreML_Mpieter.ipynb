{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb76156",
   "metadata": {},
   "source": [
    "reference:\n",
    "- https://github.com/MPieter/YOLOv4-CoreML-Converter/blob/master/convert.py\n",
    "- https://www.codeproject.com/script/Content/ViewReadingList.aspx?rlid=33\n",
    "- https://github.com/allanzelener/YAD2K/blob/master/yad2k.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48326d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import io\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import (Conv2D, GlobalAveragePooling2D, Input, Lambda,\n",
    "                          MaxPooling2D)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "\n",
    "\n",
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7340f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"yolov2.cfg\"\n",
    "weights_path = \"yolov2.weights\"\n",
    "keras_output_path = 'yolov2_keras_mpieter.h5'\n",
    "# yolov2-coco-9.mlmodel\n",
    "cml_model_path  = \"yolov2_coreml.mlmodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cb7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading weights.')\n",
    "weights_file = open(weights_path, 'rb')\n",
    "weights_header = np.ndarray(\n",
    "    shape=(4, ), dtype='int32', buffer=weights_file.read(16))\n",
    "print('Weights Header: ', weights_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_config_sections(config_file):\n",
    "    \"\"\"Convert all config sections to have unique names.\n",
    "    Adds unique suffixes to config sections for compability with configparser.\n",
    "    \"\"\"\n",
    "    section_counters = defaultdict(int)\n",
    "    output_stream = io.StringIO()\n",
    "    with open(config_file) as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('['):\n",
    "                section = line.strip().strip('[]')\n",
    "                _section = section + '_' + str(section_counters[section])\n",
    "                section_counters[section] += 1\n",
    "                line = line.replace(section, _section)\n",
    "            output_stream.write(line)\n",
    "    output_stream.seek(0)\n",
    "    return output_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parsing Darknet config.')\n",
    "unique_config_file = unique_config_sections(config_path)\n",
    "cfg_parser = configparser.ConfigParser()\n",
    "cfg_parser.read_file(unique_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = int(cfg_parser['net_0']['height'])\n",
    "image_width = int(cfg_parser['net_0']['width'])\n",
    "print(f'w ={image_width}, h={image_height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_layer = Input(shape=(image_height, image_width, 3))\n",
    "all_layers = [prev_layer]\n",
    "\n",
    "weight_decay = float(cfg_parser['net_0']['decay']) if 'net_0' in cfg_parser.sections() else 5e-4\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c128995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_to_depth_x2(x):\n",
    "    \"\"\"Thin wrapper for Tensorflow space_to_depth with block_size=2.\"\"\"\n",
    "    # Import currently required to make Lambda work.\n",
    "    # See: https://github.com/fchollet/keras/issues/5088#issuecomment-273851273\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    return tf.nn.space_to_depth(x, block_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e531e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_to_depth_x2_output_shape(input_shape):\n",
    "    \"\"\"Determine space_to_depth output shape for block_size=2.\n",
    "    Note: For Lambda with TensorFlow backend, output shape may not be needed.\n",
    "    \"\"\"\n",
    "    return (input_shape[0], input_shape[1] // 2, input_shape[2] // 2, 4 *\n",
    "            input_shape[3]) if input_shape[1] else (input_shape[0], None, None,\n",
    "                                                    4 * input_shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af90372",
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in cfg_parser.sections():\n",
    "    print('Parsing section {}'.format(section))\n",
    "    if section.startswith('convolutional'):\n",
    "        filters = int(cfg_parser[section]['filters'])\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        pad = int(cfg_parser[section]['pad'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        batch_normalize = 'batch_normalize' in cfg_parser[section]\n",
    "\n",
    "        # padding='same' is equivalent to Darknet pad=1\n",
    "        padding = 'same' if pad == 1 else 'valid'\n",
    "\n",
    "        # Setting weights.\n",
    "        # Darknet serializes convolutional weights as:\n",
    "        # [bias/beta, [gamma, mean, variance], conv_weights]\n",
    "        prev_layer_shape = K.int_shape(prev_layer)\n",
    "\n",
    "        # TODO: This assumes channel last dim_ordering.\n",
    "        weights_shape = (size, size, prev_layer_shape[-1], filters)\n",
    "        darknet_w_shape = (filters, weights_shape[2], size, size)\n",
    "        weights_size = np.product(weights_shape)\n",
    "\n",
    "        print('conv2d', 'bn'\n",
    "              if batch_normalize else '  ', activation, weights_shape)\n",
    "\n",
    "        conv_bias = np.ndarray(\n",
    "            shape=(filters, ),\n",
    "            dtype='float32',\n",
    "            buffer=weights_file.read(filters * 4))\n",
    "        count += filters\n",
    "\n",
    "        if batch_normalize:\n",
    "            bn_weights = np.ndarray(\n",
    "                shape=(3, filters),\n",
    "                dtype='float32',\n",
    "                buffer=weights_file.read(filters * 12))\n",
    "            count += 3 * filters\n",
    "\n",
    "            # TODO: Keras BatchNormalization mistakenly refers to var\n",
    "            # as std.\n",
    "            bn_weight_list = [\n",
    "                bn_weights[0],  # scale gamma\n",
    "                conv_bias,  # shift beta\n",
    "                bn_weights[1],  # running mean\n",
    "                bn_weights[2]  # running var\n",
    "            ]\n",
    "\n",
    "        conv_weights = np.ndarray(\n",
    "            shape=darknet_w_shape,\n",
    "            dtype='float32',\n",
    "            buffer=weights_file.read(weights_size * 4))\n",
    "        count += weights_size\n",
    "\n",
    "        # DarkNet conv_weights are serialized Caffe-style:\n",
    "        # (out_dim, in_dim, height, width)\n",
    "        # We would like to set these to Tensorflow order:\n",
    "        # (height, width, in_dim, out_dim)\n",
    "        # TODO: Add check for Theano dim ordering.\n",
    "        conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "        conv_weights = [conv_weights] if batch_normalize else [\n",
    "            conv_weights, conv_bias\n",
    "        ]\n",
    "\n",
    "        # Handle activation.\n",
    "        act_fn = None\n",
    "        if activation == 'leaky':\n",
    "            pass  # Add advanced activation later.\n",
    "        elif activation != 'linear':\n",
    "            raise ValueError(\n",
    "                'Unknown activation function `{}` in section {}'.format(\n",
    "                    activation, section))\n",
    "\n",
    "        # Create Conv2D layer\n",
    "        conv_layer = (Conv2D(\n",
    "            filters, (size, size),\n",
    "            strides=(stride, stride),\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            use_bias=not batch_normalize,\n",
    "            weights=conv_weights,\n",
    "            activation=act_fn,\n",
    "            padding=padding))(prev_layer)\n",
    "\n",
    "        if batch_normalize:\n",
    "            conv_layer = (BatchNormalization(\n",
    "                weights=bn_weight_list))(conv_layer)\n",
    "        prev_layer = conv_layer\n",
    "\n",
    "        if activation == 'linear':\n",
    "            all_layers.append(prev_layer)\n",
    "        elif activation == 'leaky':\n",
    "            act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n",
    "            prev_layer = act_layer\n",
    "            all_layers.append(act_layer)\n",
    "\n",
    "    elif section.startswith('maxpool'):\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        all_layers.append(\n",
    "            MaxPooling2D(\n",
    "                padding='same',\n",
    "                pool_size=(size, size),\n",
    "                strides=(stride, stride))(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('avgpool'):\n",
    "        if cfg_parser.items(section) != []:\n",
    "            raise ValueError('{} with params unsupported.'.format(section))\n",
    "        all_layers.append(GlobalAveragePooling2D()(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('route'):\n",
    "        ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]\n",
    "        layers = [all_layers[i] for i in ids]\n",
    "        if len(layers) > 1:\n",
    "            print('Concatenating route layers:', layers)\n",
    "            concatenate_layer = concatenate(layers)\n",
    "            all_layers.append(concatenate_layer)\n",
    "            prev_layer = concatenate_layer\n",
    "        else:\n",
    "            skip_layer = layers[0]  # only one layer to route\n",
    "            all_layers.append(skip_layer)\n",
    "            prev_layer = skip_layer\n",
    "\n",
    "    elif section.startswith('reorg'):\n",
    "        block_size = int(cfg_parser[section]['stride'])\n",
    "        assert block_size == 2, 'Only reorg with stride 2 supported.'\n",
    "        all_layers.append(\n",
    "            Lambda(\n",
    "                space_to_depth_x2,\n",
    "                output_shape=space_to_depth_x2_output_shape,\n",
    "                name='space_to_depth_x2')(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    elif section.startswith('region'):\n",
    "        with open('anchors.txt', 'w') as f:\n",
    "            print(cfg_parser[section]['anchors'], file=f)\n",
    "\n",
    "    elif (section.startswith('net') or section.startswith('cost') or\n",
    "          section.startswith('softmax')):\n",
    "        pass  # Configs not currently handled during model definition.\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Unsupported section header type: {}'.format(section))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create and save model.\n",
    "model = Model(inputs=all_layers[0], outputs=all_layers[-1])\n",
    "print(model.summary())\n",
    "model.save('{}'.format(keras_output_path))\n",
    "print('Saved Keras model to {}'.format(keras_output_path))\n",
    "# Check to see if all weights have been read.\n",
    "remaining_weights = len(weights_file.read()) / 4\n",
    "weights_file.close()\n",
    "print('Read {} of {} from Darknet weights.'.format(count, count +\n",
    "                                                   remaining_weights))\n",
    "if remaining_weights > 0:\n",
    "    print('Warning: {} unused weights'.format(remaining_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb695bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model, to_file='{}.png'.format(keras_output_path), show_shapes=True)\n",
    "print('Saved model plot to {}.png'.format(keras_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = model.inputs[0].name\n",
    "INPUT_NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ff94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct.convert(model = keras_output_path, inputs=[ct.ImageType(scale=1 / 255.0)])\n",
    "cml_model = ct.convert(model, inputs=[ct.ImageType(scale=1 / 255.0)], source= 'tensorflow')\n",
    "# cml_model = ct.convert(\n",
    "#     model = keras_output_path,\n",
    "#     image_input_names = [INPUT_NODE],\n",
    "#     preprocessing_args={\n",
    "#         'image_scale': 1/255.0,\n",
    "#             'is_bgr': False\n",
    "#     },\n",
    "#     minimum_ios_deployment_target='13'\n",
    "# )\n",
    "\n",
    "\n",
    "# ct.converters.onnx.convert(\n",
    "#     model=onnx_model_path,\n",
    "#     image_input_names=[INPUT_NODE],\n",
    "#     preprocessing_args={\n",
    "#         'image_scale': 1/255.0,\n",
    "#             'is_bgr': False\n",
    "#     },\n",
    "#     minimum_ios_deployment_target='13', \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cml_model.save(cml_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_and_scale_image(image_url):\n",
    "    image = Image.open(urllib.request.urlopen(image_url))\n",
    "    w,h = image.size\n",
    "    min_dim = min(w,h)\n",
    "    x0 = int((w - min_dim)/2)\n",
    "    y0 = int((h - min_dim)/2)\n",
    "    box = (x0, y0, x0 + min_dim, y0 + min_dim)\n",
    "    return image.crop(box=box).resize((416,416))\n",
    "\n",
    "def load_local_and_scale_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    w,h = image.size\n",
    "    min_dim = min(w,h)\n",
    "    x0 = int((w - min_dim)/2)\n",
    "    y0 = int((h - min_dim)/2)\n",
    "    box = (x0, y0, x0 + min_dim, y0 + min_dim)\n",
    "    return image.crop(box=box).resize((416,416))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20261438",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 13\n",
    "CELL_SIZE = int(416 / GRID_SIZE)\n",
    "BOXES_PER_CELL = 5\n",
    "\n",
    "ANCHORS = [[0.57273, 0.677385], \n",
    "           [1.87446, 2.06253], \n",
    "           [3.33843, 5.47434], \n",
    "           [7.88282, 3.52778], \n",
    "           [9.77052, 9.16828]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coco.txt', 'r') as f:\n",
    "    COCO_CLASSES = [c.strip() for c in f.readlines()]\n",
    "    \n",
    "COCO_CLASSES[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b230c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    k = np.exp(-x)\n",
    "    return 1 / (1 + k)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795690d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_preds(raw_preds: []):\n",
    "    num_classes = len(COCO_CLASSES)\n",
    "    decoded_preds = []\n",
    "    for cy in range(GRID_SIZE):\n",
    "        for cx in range(GRID_SIZE):\n",
    "            for b in range(BOXES_PER_CELL):\n",
    "                print(f'cy ={cy}--- cx ={cx} --- b={b}')\n",
    "                box_shift = b*(num_classes + 5)\n",
    "            \n",
    "                tx = float(raw_preds[0, box_shift    , cy, cx])\n",
    "                ty = float(raw_preds[0, box_shift + 1, cy, cx])\n",
    "                tw = float(raw_preds[0, box_shift + 2, cy, cx])\n",
    "                th = float(raw_preds[0, box_shift + 3, cy, cx])\n",
    "                ts = float(raw_preds[0, box_shift + 4, cy, cx])\n",
    "\n",
    "                x = (float(cx) + sigmoid(tx)) * CELL_SIZE\n",
    "                y = (float(cy) + sigmoid(ty)) * CELL_SIZE\n",
    "            \n",
    "                w = np.exp(tw) * ANCHORS[b][0] * CELL_SIZE\n",
    "                h = np.exp(th) * ANCHORS[b][1] * CELL_SIZE\n",
    "            \n",
    "                box_confidence = sigmoid(ts)\n",
    "                classes_raw = raw_preds[0, box_shift + 5:box_shift + 5 + num_classes, cy, cx]\n",
    "                classes_confidence = softmax(classes_raw)\n",
    "            \n",
    "                box_class_idx = np.argmax(classes_confidence)\n",
    "                box_class_confidence = classes_confidence[box_class_idx]\n",
    "\n",
    "                combined_confidence = box_confidence * box_class_confidence\n",
    "            \n",
    "                decoded_preds.append([box_class_idx, combined_confidence, x, y, w, h])            \n",
    "    \n",
    "    return sorted(decoded_preds, key=lambda p: p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6703929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "image = load_local_and_scale_image('persons_1.jpeg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac777d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = np.array(image)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cml_model.predict(data={'input_1': input_image})['Identity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8b8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
